# -*- coding: utf-8 -*-
"""Data_Generation Cemre - 09.05.2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Al2tuE4maldAkWz4kjb8l6EH6DUYLicI
"""

#!pip install statsmodels
# !pip install ruptures
# !pip install ipdb
# !pip install pwlf
#!pip install arch
from arch import arch_model
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_process import ArmaProcess
from statsmodels.tsa.statespace.sarimax import SARIMAX
import statsmodels.api as sm
import random
import statsmodels.api as sm
from numpy.polynomial.polynomial import Polynomial
from statsmodels.tsa.filters.hp_filter import hpfilter
import pickle

class TimeSeriesGenerator:
    def __init__(self, length=None):
        self.length = length if length is not None else 400
        self.distributions = ['ar', 'ma', 'arma','white_noise']
        self.characteristics = {'deterministic_trend_linear' : self.generate_deterministic_trend_linear,
        'deterministic_trend_cubic': self.generate_deterministic_trend_cubic,
        'deterministic_trend_quadratic': self.generate_deterministic_trend_quadratic,
        'deterministic_trend_exponential': self.generate_deterministic_trend_exponential,
        'deterministic_trend_damped': self.generate_deterministic_trend_damped,
        'stochastic_trend': self.generate_stochastic_trend,
        'single_seasonality': self.generate_single_seasonality,
        'multiple_seasonality': self.generate_multiple_seasonality,
        'single_point_anomaly' : self.generate_point_anomaly,
        'multiple_point_anomalies': self.generate_point_anomalies,
        'collective_anomalies': self.generate_collective_anomalies,
        'contextual_anomalies': self.generate_contextual_anomalies}
        self.structural_breaks = {'mean_shift': self.generate_mean_shift,
        'variance_shift': self.generate_variance_shift,
        'trend_shift': self.generate_trend_shift}

    def z_normalize(self,series):
        return (series - np.mean(series)) / np.std(series)

    def is_stationary(self, ar_params):
        # Check if AR parameters lead to stationarity
        ar_poly = np.r_[1, -ar_params]
        roots = Polynomial(ar_poly).roots()
        return np.all(np.abs(roots) > 1)

    def is_invertible(self, ma_params):
        # Check if MA parameters lead to invertibility
        ma_poly = np.r_[1, ma_params]
        roots = Polynomial(ma_poly).roots()
        return np.all(np.abs(roots) > 1)

    def extract_seasonal_part(self,series, period):
        decomposition = seasonal_decompose(series, model='additive', period=period)
        seasonal = decomposition.seasonal
        return seasonal

    def generate_ar_params(self, order_range=(1, 5), coef_range=(-0.9, 0.9)):
        while True:
            order = np.random.randint(order_range[0], order_range[1] + 1)
            coefs = np.random.uniform(coef_range[0], coef_range[1], order)
            ar = np.r_[1, -coefs]
            ma = np.array([1])
            arma_process = ArmaProcess(ar, ma)
            if arma_process.isstationary:
                break
        return order, coefs

    def generate_ar_series(self, length, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        order,coefs = self.generate_ar_params()
        ar = np.r_[1, -np.array(coefs)]  # leading 1 and negate the coefficients
        ma = np.r_[1]  # MA coefficients are just [1] for a pure AR process
        ar_process = ArmaProcess(ar, ma)
        series = ar_process.generate_sample(nsample=length)
        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_ma_params(self, order_range=(1, 5), coef_range=(-0.9, 0.9)):
        while True:
            order = np.random.randint(order_range[0], order_range[1] + 1)
            coefs = np.random.uniform(coef_range[0], coef_range[1], order)
            ma = np.r_[1, coefs]
            ar = np.array([1])
            arma_process = ArmaProcess(ar, ma)
            if arma_process.isinvertible:
                break
        return order, coefs

    def generate_ma_series(self, length, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        order,coefs = self.generate_ma_params()
        ar = np.r_[1]  # AR coefficients are just [1] for a pure MA process
        ma = np.r_[1, np.array(coefs)]  # leading 1 for the MA coefficients
        arma_process = ArmaProcess(ar, ma)
        series = arma_process.generate_sample(nsample=length)
        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_arma_params(self, order_range=(1, 2), coef_range=(-0.9, 0.9)):
        while True:
            ar_order = np.random.randint(order_range[0], order_range[1] + 1)
            ma_order = np.random.randint(order_range[0], order_range[1] + 1)
            ar_coefs = np.random.uniform(coef_range[0], coef_range[1], ar_order)
            ma_coefs = np.random.uniform(coef_range[0], coef_range[1], ma_order)
            ma = np.r_[1, ma_coefs]
            ar = np.r_[1, -ar_coefs]
            arma_process = ArmaProcess(ar, ma)
            if arma_process.isinvertible and arma_process.isstationary:
                break
        return ar_order, ma_order, ar_coefs, ma_coefs

    def generate_arma_series(self, length, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        ar_order,ma_order,ar_coefs,ma_coefs = self.generate_arma_params()
        ar = np.r_[1, -np.array(ar_coefs)]
        ma = np.r_[1, np.array(ma_coefs)]
        arma_process = ArmaProcess(ar, ma)
        series = arma_process.generate_sample(nsample=length)
        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_arima_params(self, order_range=(1, 2), coef_range=(-0.9, 0.9), d_range=(1, 2)):
        while True:
            p = np.random.randint(order_range[0], order_range[1] + 1)
            d = np.random.randint(d_range[0], d_range[1] + 1)
            q = np.random.randint(order_range[0], order_range[1] + 1)

            ar_coefs = np.random.uniform(coef_range[0], coef_range[1], p)
            ma_coefs = np.random.uniform(coef_range[0], coef_range[1], q)

            ar = np.r_[1, -ar_coefs]
            ma = np.r_[1, ma_coefs]

            arma_process = ArmaProcess(ar, ma)
            if arma_process.isstationary and arma_process.isinvertible:
                break

        return p, d, q, ar_coefs, ma_coefs

    def generate_arima_series(self, length, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        p, d, q, ar_coefs, ma_coefs = self.generate_arima_params()

        ar = np.r_[1, -ar_coefs]
        ma = np.r_[1, ma_coefs]

        arma_process = ArmaProcess(ar, ma)
        arma_sample = arma_process.generate_sample(nsample=length)

        # Integrate (difference 'd' times)
        series = arma_sample
        for _ in range(d):
            series = np.cumsum(series)

        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_ari_params(self, order_range=(1, 5), d_range = (1, 3), coef_range=(-0.9, 0.9)):
        while True:
            order = np.random.randint(order_range[0], order_range[1] + 1)
            coefs = np.random.uniform(coef_range[0], coef_range[1], order)
            d = np.random.randint(d_range[0], d_range[1] + 1)
            ar = np.r_[1, -coefs]
            ma = np.array([1])
            arma_process = ArmaProcess(ar, ma)
            if arma_process.isstationary:
                break
        return d, order, coefs

    def generate_ari_series(self, length, const=False, drift=None, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        d, order, coefs = self.generate_ari_params()
        ar = np.r_[1, -coefs]
        ma = np.array([1])
        arma_process = ArmaProcess(ar, ma)
        series = arma_process.generate_sample(nsample=length)
        for _ in range(d):
            series = np.cumsum(series)
        if const:
            if drift is None:
                drift = np.random.uniform(0.01, 0.1)
            series += drift * np.arange(length)
        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_ima_params(self, order_range=(1, 5), d_range = (1,3), coef_range=(-0.9, 0.9)):
        while True:
            order = np.random.randint(order_range[0], order_range[1] + 1)
            coefs = np.random.uniform(coef_range[0], coef_range[1], order)
            d = np.random.randint(d_range[0], d_range[1] + 1)
            ar = np.array([1])
            ma = np.r_[1, coefs]
            arma_process = ArmaProcess(ar, ma)
            if arma_process.isinvertible:
                break
        return d, order, coefs

    def generate_ima_series(self, length, const=False, drift=None, noise_scale=0.5, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        d, order, coefs = self.generate_ima_params()
        ar = np.array([1])
        ma = np.r_[1, coefs]
        arma_process = ArmaProcess(ar, ma)
        series = arma_process.generate_sample(nsample=length)
        for _ in range(d):
            series = np.cumsum(series)
        if const:
            if drift is None:
                drift = np.random.uniform(0.01, 0.1)
            series += drift * np.arange(length)
        series = series + np.random.normal(0,noise_std,length)
        series = self.z_normalize(series)
        return series

    def generate_sarima_params(self, p_range=(1, 3), d_range=(1, 2), q_range=(0, 3), P_range=(1,3), Q_range=(1,3), D_range=(1,2), coef_range=(-0.9, 0.9)):
        while True:
            p = np.random.randint(p_range[0], p_range[1] + 1)
            d = np.random.randint(d_range[0], d_range[1] + 1)
            q = np.random.randint(q_range[0], q_range[1] + 1)

            P = np.random.randint(P_range[0], P_range[1] + 1)
            D = np.random.randint(D_range[0], D_range[1] + 1)
            Q = np.random.randint(Q_range[0], Q_range[1] + 1)
            s = np.random.choice([5, 7, 12, 24, 30, 52, 90, 180])

            ar_params = np.random.uniform(coef_range[0], coef_range[1], p) if p > 0 else np.array([])
            ma_params = np.random.uniform(coef_range[0], coef_range[1], q) if q > 0 else np.array([])
            seasonal_ar_params = np.random.uniform(coef_range[0], coef_range[1], P) if P > 0 else np.array([])
            seasonal_ma_params = np.random.uniform(coef_range[0], coef_range[1], Q) if Q > 0 else np.array([])

            if (self.is_stationary(ar_params) and self.is_invertible(ma_params) and
                self.is_stationary(seasonal_ar_params) and self.is_invertible(seasonal_ma_params)):

                arma_params = np.concatenate([ar_params, ma_params, seasonal_ar_params, seasonal_ma_params])
                return (p, d, q), (P, D, Q, s), arma_params

    def generate_sarima_series(self, length, max_attempts=10, noise_std = None, noise_scale = 0.5):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        attempts = 0
        while attempts < max_attempts:
            try:
                order, seasonal_order, arma_params = self.generate_sarima_params()
                p, d, q = order
                P, D, Q, s = seasonal_order
                order_total = p + q + P + Q

                # Initial values for endog
                endog = np.random.normal(scale=noise_scale, size=length)

                variance_param = np.random.uniform(0.5, 2.0,1)
                full_params = np.concatenate([arma_params, variance_param])

                model = SARIMAX(
                    endog=endog,
                    order=order,
                    seasonal_order=seasonal_order,
                    enforce_stationarity=False,
                    enforce_invertibility=False)

                series = model.simulate(params=full_params, nsimulations=length)

                # Check for numerical issues or degenerate output
                if not np.isnan(series).any() and np.std(series) > 1e-3:
                    series = series + np.random.normal(0, noise_std, length)
                    series = self.z_normalize(series)
                    return series

            except (ValueError, np.linalg.LinAlgError):
                attempts += 1
                print(f"Attempt {attempts}/{max_attempts} failed. Retrying...")

        print("SARIMA generation failed. Returning None.")
        return None

    def generate_sarma_params(self, p_range=(1, 3), q_range=(1, 3), P_range=(1,3), Q_range=(1,3), coef_range=(-0.9, 0.9)):
        while True:
            p = np.random.randint(p_range[0], p_range[1] + 1)
            q = np.random.randint(q_range[0], q_range[1] + 1)
            d = 0

            P = np.random.randint(P_range[0], P_range[1] + 1)
            Q = np.random.randint(Q_range[0], Q_range[1] + 1)
            D = 0
            s = np.random.choice([5, 7, 12, 24, 30, 52, 90, 180])

            ar_params = np.random.uniform(coef_range[0], coef_range[1], p) if p > 0 else np.array([])
            ma_params = np.random.uniform(coef_range[0], coef_range[1], q) if q > 0 else np.array([])
            seasonal_ar_params = np.random.uniform(coef_range[0], coef_range[1], P) if P > 0 else np.array([])
            seasonal_ma_params = np.random.uniform(coef_range[0], coef_range[1], Q) if Q > 0 else np.array([])

            if (self.is_stationary(ar_params) and self.is_invertible(ma_params) and
                self.is_stationary(seasonal_ar_params) and self.is_invertible(seasonal_ma_params)):

                arma_params = np.concatenate([ar_params, ma_params, seasonal_ar_params, seasonal_ma_params])
                return (p, d, q), (P, D, Q, s), arma_params

    def generate_sarma_series(self, length, max_attempts=10, noise_std = None, noise_scale = 0.5):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        attempts = 0
        while attempts < max_attempts:
            try:
                order, seasonal_order, arma_params = self.generate_sarma_params()
                p, d, q = order
                P, D, Q, s = seasonal_order

                # Initial values for endog
                endog = np.random.normal(scale=noise_scale, size=length)

                variance_param = np.random.uniform(0.5, 2.0,1)
                full_params = np.concatenate([arma_params, variance_param])

                model = SARIMAX(
                    endog=endog,
                    order=order,
                    seasonal_order=seasonal_order,
                    enforce_stationarity=False,
                    enforce_invertibility=False)

                series = model.simulate(params=full_params, nsimulations=length)

                # Check for numerical issues or degenerate output
                if not np.isnan(series).any() and np.std(series) > 1e-3:
                    series = series + np.random.normal(0,noise_std,length)
                    series = self.z_normalize(series)
                    return series

            except (ValueError, np.linalg.LinAlgError):
                attempts += 1
                print(f"Attempt {attempts}/{max_attempts} failed. Retrying...")

        print("SARMA generation failed. Returning None.")
        return None

    def generate_white_noise(self, length, noise_std = None):
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        series = np.random.normal(0, 1, length)
        series = series + np.random.normal(0,noise_std,length)
        return series

    def generate_arch_series(self, length, alpha_range=(0.1, 0.5),omega=0.1,cumulative=False,scale_factor=1):
        alpha = np.random.uniform(*alpha_range)
        am = arch_model(None, vol='ARCH', p=1, mean='Zero')
        sim = am.simulate([omega, alpha], nobs=length)

        series = sim['data'].values * scale_factor
        if cumulative:
            series = np.cumsum(series)

        return series

    def generate_garch_series(self, length, alpha_range=(0.1, 0.3),beta_range=(0.1, 0.3),omega=0.1,cumulative=False,scale_factor=1):
        while True:
            alpha = np.random.uniform(*alpha_range)
            beta = np.random.uniform(*beta_range)
            if alpha + beta < 1:
                break  # Ensure weak stationarity of the variance

        am = arch_model(None, vol='GARCH', p=1, q=1, mean='Zero')
        sim = am.simulate([omega, alpha, beta], nobs=length)

        series = sim['data'].values * scale_factor
        if cumulative:
            series = np.cumsum(series)

        return series

    def generate_stationary_base_series(self, distribution=None):
        if distribution is None:
            distribution = np.random.choice(self.base_distributions)
        if distribution == 'white_noise':
            series = self.generate_white_noise(self.length)
        elif distribution == 'ar':
            series = self.generate_ar_series(self.length)
        elif distribution == 'ma':
            series = self.generate_ma_series(self.length)
        elif distribution == 'arma':
            series = self.generate_arma_series(self.length)

        df = pd.DataFrame({
            'data': series,
            'stationary': (np.ones(self.length)).astype(int),
            'det_lin_up': (np.zeros(self.length)).astype(int),
            'det_lin_down': (np.zeros(self.length)).astype(int),
            'det_quad': (np.zeros(self.length)).astype(int),
            'det_cubic': (np.zeros(self.length)).astype(int),
            'det_exp': (np.zeros(self.length)).astype(int),
            'det_damped': (np.zeros(self.length)).astype(int),
            'stoc_trend': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'single_seas': (np.zeros(self.length)).astype(int),
            'multiple_seas': (np.zeros(self.length)).astype(int),
            'point_anom_single': (np.zeros(self.length)).astype(int),
            'point_anom_multi' : (np.zeros(self.length)).astype(int),
            'collect_anom': (np.zeros(self.length)).astype(int),
            'context_anom': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'mean_shift': (np.zeros(self.length)).astype(int),
            'var_shift': (np.zeros(self.length)).astype(int),
            'trend_shift': (np.zeros(self.length)).astype(int),})
        return df


    def generate_non_stationary_base_series(self, distribution=None):
        if distribution is None:
            distribution = np.random.choice(self.base_distributions)
        if distribution == 'arma':
            series = self.generate_arma_series(self.length)
        elif distribution == 'arima':
            series = self.generate_arima_series(self.length)
        elif distribution == 'sarma':
            series = self.generate_sarma_series(self.length)
        elif distribution == 'sarima':
            series = self.generate_sarima_series(self.length)
        elif distribution == 'arch':
            series = self.generate_arch_series(self.length)
        elif distribution == 'garch':
            series = self.generate_garch_series(self.length)

        df = pd.DataFrame({
            'data': series,
            'stationary': (np.zeros(self.length)).astype(int),
            'det_lin_up': (np.zeros(self.length)).astype(int),
            'det_lin_down': (np.zeros(self.length)).astype(int),
            'det_quad': (np.zeros(self.length)).astype(int),
            'det_cubic': (np.zeros(self.length)).astype(int),
            'det_exp': (np.zeros(self.length)).astype(int),
            'det_damped': (np.zeros(self.length)).astype(int),
            'stoc_trend': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'single_seas': (np.zeros(self.length)).astype(int),
            'multiple_seas': (np.zeros(self.length)).astype(int),
            'point_anom_single': (np.zeros(self.length)).astype(int),
            'point_anom_multi' : (np.zeros(self.length)).astype(int),
            'collect_anom': (np.zeros(self.length)).astype(int),
            'context_anom': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'mean_shift': (np.zeros(self.length)).astype(int),
            'var_shift': (np.zeros(self.length)).astype(int),
            'trend_shift': (np.zeros(self.length)).astype(int)})
        return df

    def generate_point_anomaly(self, df, location=None, scale_factor=1, return_debug=True):
        series = df['data'].copy()
        series = self.z_normalize(series)
        num_anomalies = 1
        n = len(series)
        anomaly_info = []

        # Define candidate indices based on location
        if location == "beginning":
            candidate_range = np.arange(int(0.1 * n), int(0.3 * n))
        elif location == "middle":
            candidate_range = np.arange(int(0.4 * n), int(0.6 * n))
        elif location == "end":
            candidate_range = np.arange(int(0.7 * n), int(0.9 * n))
        else:
            candidate_range = np.arange(int(0.1 * n), int(0.9 * n))  # default safe range

        if len(candidate_range) == 0:
            raise ValueError("No valid candidate indices found for the given location.")

        # Select point anomaly index
        anomaly_indices = np.random.choice(candidate_range, num_anomalies, replace=False)

        # Inject the anomaly
        magnitude = np.random.uniform(3, 6)
        series[anomaly_indices] += np.random.choice([-1, 1], num_anomalies) * magnitude * np.std(series) * scale_factor
        anomaly_info.append((num_anomalies, anomaly_indices))

        # Update DataFrame
        df.loc[:, 'data'] = series
        df.loc[:, 'stationary'] = 0
        df.loc[:, 'point_anom_single'] = 1
        df.loc[:, 'point_anom_count'] = num_anomalies
        df.loc[:, 'point_anom_location'] = location
        df.loc[:, 'point_anom_point'] = [anomaly_indices] * len(series)

        if return_debug:
            return df, anomaly_info
        return df


    def generate_point_anomalies(self, df, scale_factor=1, return_debug = True):
        series = df['data'].copy()
        series = self.z_normalize(series)
        anomaly_info = []
        num_anomalies = max(2, np.random.randint(int(len(series)*0.01), int(len(series)*0.05)))
        magnitude = np.random.uniform(3,6)
        anomaly_indices = np.random.choice(len(series), num_anomalies, replace=False)
        series[anomaly_indices] += np.random.choice([-1, 1], num_anomalies) * magnitude * np.std(series) * scale_factor
        anomaly_indices = np.sort(anomaly_indices)
        anomaly_info.append((num_anomalies,[anomaly_indices]))
        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:,'point_anom_multi'] = 1
        df.loc[:,'point_anom_count'] = num_anomalies
        df.loc[:,'point_anom_location'] = [anomaly_indices] * len(series)
        if return_debug:
            return df, anomaly_info
        return df

    def generate_collective_anomalies(self, df, num_anomalies=1, location=None, scale_factor=1, min_distance=20, return_debug=True):
        series = df['data'].copy()
        n = len(series)
        series = self.z_normalize(series)
        anomaly_info = []
        ends = []

        # Determine candidate start regions based on location
        if location == "beginning":
            candidate_range = np.arange(int(0.1 * n), int(0.3 * n))
        elif location == "middle":
            candidate_range = np.arange(int(0.4 * n), int(0.6 * n))
        elif location == "end":
            candidate_range = np.arange(int(0.7 * n), int(0.9 * n))
        else:
            candidate_range = np.arange(int(0.1 * n), int(0.85 * n))  # Safe zone to avoid end

        # Select non-overlapping anomaly start points
        selected_starts = []
        candidates = candidate_range.copy()
        while len(selected_starts) < num_anomalies and len(candidates) > 0:
            start = np.random.choice(candidates)
            selected_starts.append(start)
            candidates = candidates[np.abs(candidates - start) >= min_distance]

        # Apply collective anomalies
        for start in selected_starts:
            max_length = n - start - int(n * 0.05)  # Leave room to return to normal
            anomaly_length = np.random.randint(int(n * 0.05), min(int(n * 0.09), max_length))
            end = min(n, start + anomaly_length)
            ends.append(end)
            magnitude = np.random.uniform(1.5, 2.5)
            sign = np.random.choice([-1, 1])
            series[start:end] += sign * magnitude * np.std(series) * scale_factor
            anomaly_info.append((start, end))


        ends = np.sort(ends)
        selected_starts = np.sort(selected_starts)
        # Update DataFrame
        df.loc[:, 'data'] = series
        df.loc[:, 'stationary'] = 0
        df.loc[:, 'collect_anom'] = 1
        df.loc[:, 'collect_anom_count'] = len(selected_starts)
        df.loc[:, 'collect_anom_location'] = location
        df.loc[:, 'collect_anom_start'] = [selected_starts] * len(series)
        df.loc[:, 'collect_anom_end'] = [ends] * len(series)

        if return_debug:
            return df, anomaly_info
        return df


    def generate_contextual_anomalies(self, df, period=None, scale_factor=1, return_debug=True):
        series = df['data'].copy()
        n = len(series)
        anomaly_info = []

        # Decide on the seasonal period
        if period is None:
            min_period = 5
            max_period = n // 6
            periods = [p for p in [5, 7, 12, 24, 30, 52, 90, 180] if min_period <= p <= max_period]
            period = random.choice(periods)

        # Add strong seasonality to make context visible
        amplitude = np.std(series) * np.random.uniform(1.5, 3)
        seasonality = amplitude * np.sin(2 * np.pi * np.arange(n) / period)
        series += seasonality * scale_factor

        # Use clean sine to find contextual targets
        pure_seasonality = np.sin(2 * np.pi * np.arange(n) / period)
        peaks = np.where((pure_seasonality[1:-1] > pure_seasonality[:-2]) & (pure_seasonality[1:-1] > pure_seasonality[2:]))[0] + 1
        valleys = np.where((pure_seasonality[1:-1] < pure_seasonality[:-2]) & (pure_seasonality[1:-1] < pure_seasonality[2:]))[0] + 1
        candidate_indices = np.concatenate([peaks, valleys])

        if len(candidate_indices) == 0:
            return df  # No suitable context found

        # Select center of anomaly region
        anomaly_center = np.random.choice(candidate_indices)
        anomaly_length = np.random.randint(3, 6)
        start = max(0, anomaly_center - anomaly_length // 2)
        end = min(n, start + anomaly_length)

        # Choose anomaly type and magnitude
        anomaly_type = np.random.choice(['flatten_peak', 'deepen_valley'])
        magnitude = np.random.uniform(3, 6) * scale_factor

        # Apply the anomaly visibly
        if anomaly_type == 'flatten_peak':
            peak_level = np.max(series[start:end])
            series[start:end] = peak_level - magnitude
        elif anomaly_type == 'deepen_valley':
            valley_level = np.min(series[start:end])
            series[start:end] = valley_level + magnitude

        anomaly_info.append((start, end))

        # Labeling
        df.loc[:, 'data'] = series
        df.loc[:, 'stationary'] = 0
        df.loc[:, 'context_anom'] = 1
        df.loc[:, 'context_anom_start'] = start
        df.loc[:, 'context_anom_end'] = end

        if return_debug:
            return df, anomaly_info
        return df


    def generate_deterministic_trend_linear(self, df, sign = None, slope= None, noise_std = None, intercept = 1, scale_factor = 1):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        slope_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
        slope = slope if slope is not None else random.choice(slope_range)*sign
        trend = intercept + slope * np.arange(len(series)) + np.random.normal(0, noise_std, len(series))
        series += trend * scale_factor
        series = self.z_normalize(series)
        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        if sign > 0:
            df.loc[:,'det_lin_up'] = 1
        else:
            df.loc[:,'det_lin_down'] = 1
        return df

    def generate_deterministic_trend_quadratic(self, df, sign=None, a=None, b=None, c=None, noise_std=None, scale_factor=1):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)

        if len(series) <= 100:
            scale_factor = 5
        else:
            scale_factor = 1

        a = a if a is not None else sign * random.uniform(0.0005, 0.002)
        b = b if b is not None else sign * random.uniform(0.02, 0.05)
        c = c if c is not None else sign * random.uniform(1, 3)

        t = np.arange(len(series))
        trend = (a * t**2 + b * t + c) * scale_factor + np.random.normal(0, noise_std, len(series))

        series += trend
        series = self.z_normalize(series)
        df.loc[:, 'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:, 'det_quad'] = 1
        return df


    def generate_deterministic_trend_cubic(self, df, sign=None, a=None, b=None, c=None, d=None, noise_std=None, scale_factor=1):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)

        if len(series) <= 100:
            scale_factor = 5
        else:
            scale_factor = 1

        a = a if a is not None else sign * random.uniform(0.00001, 0.00005)
        b = b if b is not None else sign * random.uniform(-0.005, 0.005)
        c = c if c is not None else sign * random.uniform(0.05, 0.2)
        d = d if d is not None else sign * random.uniform(1, 3)

        t = np.arange(len(series))
        trend = (a * t**3 + b * t**2 + c * t + d) * scale_factor + np.random.normal(0, noise_std, len(series))

        series += trend
        series = self.z_normalize(series)
        df.loc[:, 'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:, 'det_cubic'] = 1
        return df

    def generate_deterministic_trend_exponential(self, df, sign=None, a=None, b=None, noise_std=None, scale_factor=1):
        if sign is None:
            raise ValueError("sign must be provided (+1 or -1)")

        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.01, 0.1)

        if len(series) == 100:
            scale_factor = 5
        else:
            scale_factor = 1

        a = a if a is not None else random.uniform(0.5, 2.0)
        b = b if b is not None else sign * random.uniform(0.005, 0.02)

        t = np.arange(len(series))
        trend = a * np.exp(b * t) * scale_factor
        noise = np.random.normal(0, noise_std, len(series))

        series += trend + noise
        series = self.z_normalize(series)

        df.loc[:, 'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:, 'det_exp'] = 1
        return df

    def generate_deterministic_trend_damped(self, df, sign=None, a=None, b=None, damping_rate=None, noise_std=None, scale_factor=1):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.1, 1.5)
        a = a if a is not None else sign * random.uniform(0.5, 2.0)
        b = b if b is not None else random.uniform(0.0, 1.0)
        damping_rate = damping_rate if damping_rate is not None else random.uniform(0.01, 0.05)
        t = np.arange(len(series))
        noise = np.random.normal(0, noise_std, len(series))
        trend = (a * t + b) * np.exp(-damping_rate * t) * scale_factor + noise
        series += trend
        series = self.z_normalize(series)
        df.loc[:, 'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:, 'det_damped'] = 1
        return df

    def generate_stochastic_trend(self, kind='rw', const=False, drift=None, noise_std=1.0):
        t = np.arange(self.length)
        noise = np.random.normal(0, noise_std, self.length)

        if kind == 'rw':
            series = np.cumsum(noise)
            series = self.z_normalize(series)

        elif kind == 'rwd':
            if drift is None:
                drift = np.random.uniform(0.01, 0.1)
            series = drift * t + np.cumsum(noise)
            series = self.z_normalize(series)

        elif kind == 'ari':
            series = self.generate_ari_series(length=self.length)
            series = self.z_normalize(series)

        elif kind == 'ima':
            series = self.generate_ima_series(length=self.length)
            series = self.z_normalize(series)

        elif kind == 'arima':
            series = self.generate_arima_series(length=self.length)
            series = self.z_normalize(series)

        else:
            raise ValueError("Invalid kind. Choose from 'rw', 'rwd', 'ari', 'ima', or 'arima'.")

        df = pd.DataFrame({
            'data': series,
            'stationary': (np.zeros(self.length)).astype(int),
            'det_lin_up': (np.zeros(self.length)).astype(int),
            'det_lin_down': (np.zeros(self.length)).astype(int),
            'det_quad': (np.zeros(self.length)).astype(int),
            'det_cubic': (np.zeros(self.length)).astype(int),
            'det_exp': (np.zeros(self.length)).astype(int),
            'det_damped': (np.zeros(self.length)).astype(int),
            'stoc_trend': (np.ones(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'single_seas': (np.zeros(self.length)).astype(int),
            'multiple_seas': (np.zeros(self.length)).astype(int),
            'point_anom_single': (np.zeros(self.length)).astype(int),
            'point_anom_multi' : (np.zeros(self.length)).astype(int),
            'collect_anom': (np.zeros(self.length)).astype(int),
            'context_anom': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'mean_shift': (np.zeros(self.length)).astype(int),
            'var_shift': (np.zeros(self.length)).astype(int),
            'trend_shift': (np.zeros(self.length)).astype(int),})
        return df

    def generate_single_seasonality(self, df, period=None, amplitude=None, noise_std=None, scale_factor = 3):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.01, 0.05)
        min_period = 5
        max_period = len(series) // 6  # Ensure at least 6 cycles
        periods = [p for p in [5, 7, 12, 24, 30, 52, 90, 180] if min_period <= p <= max_period]
        period = period if period is not None else random.choice(periods)
        amplitude = amplitude if amplitude is not None else np.std(series) * np.random.uniform(0.5, 2.5)
        seasonality = (amplitude * np.sin(2 * np.pi * np.arange(len(series)) / period) + np.random.normal(0, noise_std, size = len(series)))
        series += seasonality * scale_factor
        series = self.z_normalize(series)
        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:,'single_seas'] = 1
        return period, df

    def generate_seasonality_from_base_series(self, kind = None):
        if kind == 'sarma':
            series = self.generate_sarma_series(self.length)
            series = self.z_normalize(series)
        if kind == 'sarima':
            series = self.generate_sarima_series(self.length)
            series = self.z_normalize(series)

        df = pd.DataFrame({
            'data': series,
            'stationary': (np.zeros(self.length)).astype(int),
            'det_lin_up': (np.zeros(self.length)).astype(int),
            'det_lin_down': (np.zeros(self.length)).astype(int),
            'det_quad': (np.zeros(self.length)).astype(int),
            'det_cubic': (np.zeros(self.length)).astype(int),
            'det_exp': (np.zeros(self.length)).astype(int),
            'det_damped': (np.zeros(self.length)).astype(int),
            'stoc_trend': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'single_seas': (np.ones(self.length)).astype(int),
            'multiple_seas': (np.zeros(self.length)).astype(int),
            'point_anom_single': (np.zeros(self.length)).astype(int),
            'point_anom_multi' : (np.zeros(self.length)).astype(int),
            'collect_anom': (np.zeros(self.length)).astype(int),
            'context_anom': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'mean_shift': (np.zeros(self.length)).astype(int),
            'var_shift': (np.zeros(self.length)).astype(int),
            'trend_shift': (np.zeros(self.length)).astype(int),
            'seas_shift': (np.zeros(self.length)).astype(int),})
        return df

    def generate_multiple_seasonality(self, df, num_components=2, periods=None, amplitudes=None, noise_std=None, scale_factor=3):
        series = df['data'].copy()
        noise_std = noise_std if noise_std is not None else np.random.uniform(0.01, 0.05)
        min_period = 5
        max_period = len(series) // 6
        valid_periods = [p for p in [5, 7, 12, 24, 30, 52, 90, 180] if min_period <= p <= max_period]

        if periods is None:
            periods = random.sample(valid_periods, min(num_components, len(valid_periods)))

        if amplitudes is None:
            base_std = np.std(series)
            amplitudes = [base_std * np.random.uniform(0.5, 2.0) for _ in periods]

        for period, amplitude in zip(periods, amplitudes):
            seasonal_component = amplitude * np.sin(2 * np.pi * np.arange(len(series)) / period)
            seasonal_component += np.random.normal(0, noise_std, size=len(series))
            series += seasonal_component * scale_factor
            series = self.z_normalize(series)

        df.loc[:, 'data'] = series
        df.loc[:, 'multiple_seas'] = 1
        df.loc[:,'stationary'] = 0
        return periods, df

    def generate_volatility(self, kind = None):
        if kind == 'arch':
            series = self.generate_arch_series(self.length)
            series = self.z_normalize(series)
        elif kind == 'garch':
            series = self.generate_garch_series(self.length)
            series = self.z_normalize(series)

        df = pd.DataFrame({
            'data': series,
            'stationary': (np.zeros(self.length)).astype(int),
            'det_lin_up': (np.zeros(self.length)).astype(int),
            'det_lin_down': (np.zeros(self.length)).astype(int),
            'det_quad': (np.zeros(self.length)).astype(int),
            'det_cubic': (np.zeros(self.length)).astype(int),
            'det_exp': (np.zeros(self.length)).astype(int),
            'det_damped': (np.zeros(self.length)).astype(int),
            'stoc_trend': (np.zeros(self.length)).astype(int),
            'volatility': (np.zeros(self.length)).astype(int),
            'single_seas': (np.zeros(self.length)).astype(int),
            'multiple_seas': (np.zeros(self.length)).astype(int),
            'point_anom_single': (np.zeros(self.length)).astype(int),
            'point_anom_multi' : (np.zeros(self.length)).astype(int),
            'collect_anom': (np.zeros(self.length)).astype(int),
            'context_anom': (np.zeros(self.length)).astype(int),
            'volatility': (np.ones(self.length)).astype(int),
            'mean_shift': (np.zeros(self.length)).astype(int),
            'var_shift': (np.zeros(self.length)).astype(int),
            'trend_shift': (np.zeros(self.length)).astype(int),
            'seas_shift': (np.zeros(self.length)).astype(int),})
        return df


    def generate_mean_shift(self, df, signs=None, location=None, num_breaks=1, scale_factor=1, seasonal_period=None, min_distance=20, return_debug=True):
        series = df['data'].copy()
        n = len(series)
        residuals, trend = hpfilter(series, lamb=11000)
        created_breaks = []
        shifts_info = []

        # Decide break points
        if num_breaks == 1 and location in ["beginning", "middle", "end"]:
            if location == "beginning":
                break_points = [np.random.randint(int(0.1 * n), int(0.3 * n))]
            elif location == "middle":
                break_points = [np.random.randint(int(0.4 * n), int(0.6 * n))]
            elif location == "end":
                break_points = [np.random.randint(int(0.7 * n), int(0.9 * n))]
        else:
            candidates = np.arange(int(0.1 * n), int(0.9 * n))
            break_points = []
            while len(break_points) < num_breaks and len(candidates) > 0:
                point = np.random.choice(candidates)
                break_points.append(point)
                candidates = candidates[np.abs(candidates - point) >= min_distance]
            break_points = sorted(break_points)

        # Apply shifts
        for i, break_point in enumerate(break_points):
            magnitude = np.random.uniform(0.5, 2)
            if signs is not None and i < len(signs):
                sign = signs[i]
            else:
                sign = np.random.choice([-1, 1])
            level_shift = sign * magnitude

            trend[break_point:] += level_shift * scale_factor
            created_breaks.append(break_point)
            shifts_info.append((num_breaks, break_points, location))

        # Reconstruct series
        series = residuals + trend + np.random.normal(0, 1, size=n)
        series = self.z_normalize(series)
        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:,'mean_shift'] = 1
        df.loc[:,'mean_shift_count'] = num_breaks
        df.loc[:, 'mean_shift_location'] = location
        df.loc[:, 'mean_shift_point'] = [break_points] * len(series)
        if return_debug:
            return df, shifts_info
        return df


    def generate_variance_shift(self, df, signs=None, location=None, num_breaks=1, scale_factor=1, seasonal_period=None, min_distance=20, return_debug=True):
        series = df['data'].copy()
        n = len(series)
        residuals, trend = hpfilter(series, lamb=11000)
        created_breaks = []
        shifts_info = []

        if num_breaks == 1 and location in ["beginning", "middle", "end"]:
            if location == "beginning":
                break_points = [np.random.randint(int(0.1 * n), int(0.3 * n))]
            elif location == "middle":
                break_points = [np.random.randint(int(0.4 * n), int(0.6 * n))]
            elif location == "end":
                break_points = [np.random.randint(int(0.7 * n), int(0.9 * n))]
        else:
            candidates = np.arange(int(0.1 * n), int(0.9 * n))
            break_points = []
            while len(break_points) < num_breaks and len(candidates) > 0:
                point = np.random.choice(candidates)
                break_points.append(point)
                candidates = candidates[np.abs(candidates - point) >= min_distance]
            break_points = sorted(break_points)

        for i, break_point in enumerate(break_points):
            variance_factor = np.random.uniform(1.5, 3)
            if signs is not None and i < len(signs):
                sign = signs[i]
            else:
                sign = np.random.choice([-1, 1])

            if sign == 1:
                residuals[break_point:] *= variance_factor * scale_factor
            elif sign == -1:
                residuals[break_point:] /= variance_factor * scale_factor

            created_breaks.append((break_point))
            shifts_info.append((num_breaks, break_points, location))

        series = residuals + trend + np.random.normal(0, 1, size=n)
        series = self.z_normalize(series)

        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:, 'variance_shift'] = 1
        df.loc[:, 'variance_shift_count'] = num_breaks
        df.loc[:, 'variance_shift_location'] = location
        df.loc[:, 'variance_shift_points'] = [break_points] * len(series)
        if return_debug:
            return df, shifts_info
        return df

    def generate_trend_shift(self, df, signs=None, location=None, num_breaks=1, change_type=None, seasonal_period=None, scale_factor=1, min_distance=20, return_debug=True):
        series = df['data'].copy()
        n = len(series)
        noise = np.random.uniform(0.1, 1.5)
        residuals, _ = hpfilter(series, lamb=11000)
        created_breaks = []
        shifts_info = []

        # Define slope pools and choose base slope
        positive_slopes = [0.1, 0.2, 0.3, 0.4, 0.5]
        negative_slopes = [-0.1, -0.2, -0.3, -0.4, -0.5]
        slope_change_factor = np.random.uniform(3, 6)

        if seasonal_period:
            seasonality = np.sin(2 * np.pi * np.arange(n) / seasonal_period)
        else:
            seasonality = np.zeros(n)

        # Decide break points
        if num_breaks == 1 and location in ["beginning", "middle", "end"]:
            if location == "beginning":
                break_points = [np.random.randint(int(0.1 * n), int(0.3 * n))]
            elif location == "middle":
                break_points = [np.random.randint(int(0.4 * n), int(0.6 * n))]
            elif location == "end":
                break_points = [np.random.randint(int(0.7 * n), int(0.9 * n))]
        else:
            candidates = np.arange(int(0.1 * n), int(0.9 * n))
            break_points = []
            while len(break_points) < num_breaks and len(candidates) > 0:
                point = np.random.choice(candidates)
                break_points.append(point)
                candidates = candidates[np.abs(candidates - point) >= min_distance]
            break_points = sorted(break_points)

        # Initialize trend array
        trend = np.zeros(n)
        if signs[0] > 0:
            current_slope = np.random.choice(positive_slopes)
        if signs[0] < 0:
            current_slope = np.random.choice(negative_slopes)
        current_level = 0
        prev_point = 0

        for i, break_point in enumerate(break_points + [n]):  # Include end of series
            segment_length = break_point - prev_point
            segment_trend = current_level + current_slope * np.arange(segment_length)
            trend[prev_point:break_point] = segment_trend
            current_level = segment_trend[-1]

            if break_point == n:
                break

            if signs is not None and i < len(signs):
                sign = signs[i]
            else:
                sign = np.random.choice([-1, 1])

            if change_type == 'direction_change':
                current_slope = -current_slope
            elif change_type == 'magnitude_change':
                current_slope = current_slope * slope_change_factor * scale_factor
            elif change_type == 'direction_and_magnitude_change':
                current_slope = -current_slope * slope_change_factor * scale_factor
            else:
                raise ValueError("Invalid change_type.")

            created_breaks.append((break_point, n))
            shifts_info.append((num_breaks, break_points, location, change_type))
            prev_point = break_point

        # Final series
        series = trend + residuals + seasonality + np.random.normal(0, noise, size=n)
        series = self.z_normalize(series)

        # Update dataframe
        df.loc[:,'data'] = series
        df.loc[:,'stationary'] = 0
        df.loc[:,'trend_shift'] = 1
        df.loc[:,'trend_shift_count'] = num_breaks
        df.loc[:,'trend_shift_location'] = location
        df.loc[:,'trend_shift_points'] = [break_points] * len(series)
        if return_debug:
            return df, shifts_info
        return df


    def save_data(dataframes, filename):
        saved_data = [a.astype({
            'data': np.float32,
            "mean_shift": bool,
            "variance_shift": bool,
            "trend_shift": bool,
            "anomaly": bool,
        }) for a in dataframes]

        with open(filename, "wb") as f:
            pickle.dump(saved_data, f)

#Base stationary
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('white_noise')
plt.figure(figsize = (15,5))
plt.plot(series['data'])
# plt.show()

#Deterministic linear
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('white_noise')
series_trend = ts.generate_deterministic_trend_linear(series,-1)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Deterministic quadratic
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('white_noise')
series_trend = ts.generate_deterministic_trend_quadratic(series,-1)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Deterministic cubic
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend = ts.generate_deterministic_trend_cubic(series,1)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Deterministic exponential
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('arma')
series_trend = ts.generate_deterministic_trend_exponential(series,-1)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Deterministic damped
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('white_noise')
series_trend = ts.generate_deterministic_trend_damped(series,-1)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Stochastic ARI
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_stochastic_trend(kind='ari')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Stochastic IMA
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_stochastic_trend(kind='ima')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Stochastic ARIMA
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_stochastic_trend(kind='arima')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Stochastic random walk
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_stochastic_trend(kind='rw')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Stochastic random walk with drift
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_stochastic_trend(kind='rwd')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Single seasonality
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('white_noise')
period, series_trend = ts.generate_single_seasonality(series)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Multiple seasonality
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
period, series_trend = ts.generate_multiple_seasonality(series)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Seasonality from series(SARMA)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_seasonality_from_base_series(kind = 'sarma')
plt.figure(figsize = (15,5))
plt.plot(series['data'])
# plt.show()

#Seasonality from series(SARIMA)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_seasonality_from_base_series(kind = 'sarima')
plt.figure(figsize = (15,5))
plt.plot(series['data'])
# plt.show()

#Point anomaly(only 1)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend,info = ts.generate_point_anomaly(series, location = 'beginning')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Point anomalies (multiple)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend,info = ts.generate_point_anomalies(series)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Collective anomalies
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_collective_anomalies(series, location = 'middle')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Contextual anomalies(only for seasonal series)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_contextual_anomalies(series)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Mean shift(single)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_mean_shift(series, location = 'middle')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Mean shift(multiple)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_mean_shift(series, num_breaks = 2)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Mean shift(multiple)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_mean_shift(series, num_breaks = 3)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Variance shift (single)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_variance_shift(series, location = 'middle')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Variance shift (multiple)
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_variance_shift(series, num_breaks = 2)
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Trend shift
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_trend_shift(series, signs = [1], location = 'end', change_type = 'direction_change')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Trend shift
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_trend_shift(series, signs = [-1], location = 'middle', change_type = 'magnitude_change')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Trend shift
ts = TimeSeriesGenerator(length = 500)
series = ts.generate_stationary_base_series('ar')
series_trend, info = ts.generate_trend_shift(series, signs = [1], location = 'middle', change_type = 'direction_and_magnitude_change')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Volatile series (ARCH)
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_volatility(kind = 'arch')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()

#Volatile series (GARCH)
ts = TimeSeriesGenerator(length = 500)
series_trend = ts.generate_volatility(kind = 'garch')
plt.figure(figsize = (15,5))
plt.plot(series_trend['data'])
# plt.show()